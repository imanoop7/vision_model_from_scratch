{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mZFoQqQSVdYF"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AgDARyfeVmrJ"
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head self-attention mechanism for processing sequences.\n",
    "    \n",
    "    Args:\n",
    "        n_heads: Number of attention heads\n",
    "        embed_dim: Dimension of the embedding space\n",
    "        in_proj_bias: Whether to include bias in input projection\n",
    "        out_proj_bias: Whether to include bias in output projection\n",
    "    \"\"\"\n",
    "    def __init__(self, n_heads, embed_dim, in_proj_bias=True, out_proj_bias=True):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.in_proj = nn.Linear(embed_dim, 3 * embed_dim, bias=in_proj_bias)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=out_proj_bias)\n",
    "        self.d_head = embed_dim // n_heads\n",
    "\n",
    "    def forward(self, x, causal_mask=False):\n",
    "        \"\"\"\n",
    "        Forward pass for multi-head self-attention.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, sequence_length, embedding_dim)\n",
    "            causal_mask: Whether to apply causal masking for autoregressive generation\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, sequence_length, embedding_dim)\n",
    "        \"\"\"\n",
    "        batch_size, sequence_length, embedding_dim = x.shape\n",
    "\n",
    "        # Project input to query, key, and value\n",
    "        interim_shape = (batch_size, sequence_length, self.n_heads, self.d_head)\n",
    "        query, key, value = self.in_proj(x).chunk(3, dim=-1)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        query = query.view(interim_shape).transpose(1, 2)\n",
    "        key = key.view(interim_shape).transpose(1, 2)\n",
    "        value = value.view(interim_shape).transpose(1, 2)\n",
    "\n",
    "        # Compute attention scores\n",
    "        attention_weights = query @ key.transpose(-1, -2)\n",
    "\n",
    "        # Apply causal mask if specified\n",
    "        if causal_mask:\n",
    "            mask = torch.ones_like(attention_weights, dtype=torch.bool).triu(1)\n",
    "            attention_weights.masked_fill_(mask, -torch.inf)\n",
    "\n",
    "        # Scale and normalize attention weights\n",
    "        attention_weights = attention_weights / math.sqrt(self.d_head)\n",
    "        attention_weights = F.softmax(attention_weights, dim=-1)\n",
    "\n",
    "        # Apply attention to values\n",
    "        output = attention_weights @ value\n",
    "        \n",
    "        # Reshape output back to original dimensions\n",
    "        output = output.transpose(1, 2)\n",
    "        output = output.reshape(batch_size, sequence_length, embedding_dim)\n",
    "        \n",
    "        # Final output projection\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3K1eA6xMFMlF"
   },
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention block for spatial feature processing in convolutional networks.\n",
    "    \n",
    "    Args:\n",
    "        channels: Number of input and output channels\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.groupnorm = nn.GroupNorm(32, channels)\n",
    "        self.attention = SelfAttention(1, channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply self-attention to spatial features.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, channels, height, width)\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, channels, height, width)\n",
    "        \"\"\"\n",
    "        residual = x.clone()\n",
    "\n",
    "        # Normalize features\n",
    "        x = self.groupnorm(x)\n",
    "\n",
    "        batch_size, channels, height, width = x.shape\n",
    "\n",
    "        # Reshape spatial dimensions into sequence for attention\n",
    "        # (batch_size, channels, height, width) -> (batch_size, channels, height * width)\n",
    "        x = x.view((batch_size, channels, height * width))\n",
    "\n",
    "        # Transpose to sequence format\n",
    "        # (batch_size, channels, height * width) -> (batch_size, height * width, channels)\n",
    "        x = x.transpose(-1, -2)\n",
    "\n",
    "        # Apply self-attention without causal masking\n",
    "        # (batch_size, height * width, channels) -> (batch_size, height * width, channels)\n",
    "        x = self.attention(x)\n",
    "\n",
    "        # Transpose back to channel-first format\n",
    "        # (batch_size, height * width, channels) -> (batch_size, channels, height * width)\n",
    "        x = x.transpose(-1, -2)\n",
    "\n",
    "        # Reshape back to spatial dimensions\n",
    "        # (batch_size, channels, height * width) -> (batch_size, channels, height, width)\n",
    "        x = x.view((batch_size, channels, height, width))\n",
    "\n",
    "        # Add residual connection\n",
    "        x += residual\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "psk4Ao2fP612"
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual block with group normalization and convolutional layers.\n",
    "    \n",
    "    Args:\n",
    "        in_channels: Number of input channels\n",
    "        out_channels: Number of output channels\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.groupnorm1 = nn.GroupNorm(32, in_channels)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        self.groupnorm2 = nn.GroupNorm(32, out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        # Use identity mapping if channels match, otherwise use 1x1 convolution\n",
    "        if in_channels == out_channels:\n",
    "            self.residual_layer = nn.Identity()\n",
    "        else:\n",
    "            self.residual_layer = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through residual block.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, in_channels, height, width)\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, out_channels, height, width)\n",
    "        \"\"\"\n",
    "        residual = x.clone()\n",
    "\n",
    "        # First normalization and convolution\n",
    "        x = self.groupnorm1(x)\n",
    "        x = F.selu(x)\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        # Second normalization and convolution\n",
    "        x = self.groupnorm2(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        # Add residual connection\n",
    "        return x + self.residual_layer(residual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yA9qQ6CDQGNB"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Sequential):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder (VAE) Encoder with hierarchical feature extraction.\n",
    "    \n",
    "    Progressively downsamples input images while increasing channel depth,\n",
    "    culminating in a latent space representation with mean and log variance.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            # Initial convolution: (batch_size, 3, height, width) -> (batch_size, 128, height, width)\n",
    "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
    "\n",
    "            # First residual block: (batch_size, 128, height, width) -> (batch_size, 128, height, width)\n",
    "            ResidualBlock(128, 128),\n",
    "\n",
    "            # Downsample by 2: (batch_size, 128, height, width) -> (batch_size, 128, height/2, width/2)\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=0),\n",
    "\n",
    "            # Increase channels to 256: (batch_size, 128, height/2, width/2) -> (batch_size, 256, height/2, width/2)\n",
    "            ResidualBlock(128, 256),\n",
    "\n",
    "            # Maintain resolution: (batch_size, 256, height/2, width/2) -> (batch_size, 256, height/2, width/2)\n",
    "            ResidualBlock(256, 256),\n",
    "\n",
    "            # Downsample by 2: (batch_size, 256, height/2, width/2) -> (batch_size, 256, height/4, width/4)\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=0),\n",
    "\n",
    "            # Increase channels to 512: (batch_size, 256, height/4, width/4) -> (batch_size, 512, height/4, width/4)\n",
    "            ResidualBlock(256, 512),\n",
    "\n",
    "            # Maintain resolution: (batch_size, 512, height/4, width/4) -> (batch_size, 512, height/4, width/4)\n",
    "            ResidualBlock(512, 512),\n",
    "\n",
    "            # Downsample by 2: (batch_size, 512, height/4, width/4) -> (batch_size, 512, height/8, width/8)\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=0),\n",
    "\n",
    "            # Deep processing at lowest resolution\n",
    "            # (batch_size, 512, height/8, width/8) -> (batch_size, 512, height/8, width/8)\n",
    "            ResidualBlock(512, 512),\n",
    "            ResidualBlock(512, 512),\n",
    "            ResidualBlock(512, 512),\n",
    "\n",
    "            # Apply spatial attention: (batch_size, 512, height/8, width/8) -> (batch_size, 512, height/8, width/8)\n",
    "            AttentionBlock(512),\n",
    "\n",
    "            # Final residual processing: (batch_size, 512, height/8, width/8) -> (batch_size, 512, height/8, width/8)\n",
    "            ResidualBlock(512, 512),\n",
    "\n",
    "            # Normalize and activate: (batch_size, 512, height/8, width/8) -> (batch_size, 512, height/8, width/8)\n",
    "            nn.GroupNorm(32, 512),\n",
    "            nn.SiLU(),\n",
    "\n",
    "            # Project to latent space: (batch_size, 512, height/8, width/8) -> (batch_size, 8, height/8, width/8)\n",
    "            nn.Conv2d(512, 8, kernel_size=3, padding=1),\n",
    "\n",
    "            # Final projection: (batch_size, 8, height/8, width/8) -> (batch_size, 8, height/8, width/8)\n",
    "            nn.Conv2d(8, 8, kernel_size=1, padding=0)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Encode input image to latent space representation.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, 3, height, width)\n",
    "            \n",
    "        Returns:\n",
    "            Latent representation of shape (batch_size, 4, height/8, width/8)\n",
    "        \"\"\"\n",
    "        # Apply each module in sequence\n",
    "        for module in self:\n",
    "            # Add padding for downsampling convolutions to maintain proper dimensions\n",
    "            if isinstance(module, nn.Conv2d) and module.stride == (2, 2):\n",
    "                x = F.pad(x, (0, 1, 0, 1))  # Pad right and bottom\n",
    "            x = module(x)\n",
    "\n",
    "        # Split output into mean and log variance for reparameterization\n",
    "        # (batch_size, 8, height/8, width/8) -> 2 tensors of shape (batch_size, 4, height/8, width/8)\n",
    "        mean, log_variance = torch.chunk(x, 2, dim=1)\n",
    "\n",
    "        # Clamp log variance to prevent numerical instability\n",
    "        log_variance = torch.clamp(log_variance, -30, 20)\n",
    "\n",
    "        # Reparameterization trick: sample from N(mean, variance)\n",
    "        standard_deviation = torch.exp(0.5 * log_variance)\n",
    "        epsilon = torch.randn_like(standard_deviation)\n",
    "        latent = mean + epsilon * standard_deviation\n",
    "\n",
    "        # Scale latent representation by constant factor for stable training\n",
    "        latent *= 0.18215\n",
    "\n",
    "        return latent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DGYblMzuQKrP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
